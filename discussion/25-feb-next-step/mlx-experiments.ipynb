{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX Experiments ‚Äî Hands-On LLM Exploration\n",
    "\n",
    "**Hardware:** M3 Pro, 36 GB unified memory  \n",
    "**Goal:** See every concept from our React app with REAL models  \n",
    "\n",
    "Run each cell, read the comments, observe the output. Tweak and re-run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Run this once. If you already installed, skip to Experiment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install mlx mlx-lm transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify installation\nimport mlx.core as mx\nprint(f\"MLX version: {mx.__version__}\")\nprint(f\"Backend: {mx.default_device()}\")\n\nimport mlx_lm\nprint(f\"MLX-LM installed ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Download Models (One-Time)\n\nDownloads models to `./models/` so they survive cache cleanups. Run once ‚Äî skip after that."
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import snapshot_download\nimport os\n\nMODELS_DIR = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"models\")\n\n# Download Qwen 8B (~5 GB) ‚Äî our primary model\nqwen_path = os.path.join(MODELS_DIR, \"Qwen3-8B-4bit\")\nif not os.path.exists(qwen_path):\n    print(\"Downloading Qwen3-8B-4bit (~5 GB)... grab a coffee ‚òï\")\n    snapshot_download(\n        repo_id=\"mlx-community/Qwen3-8B-4bit\",\n        local_dir=qwen_path,\n    )\n    print(f\"Saved to: {qwen_path}\")\nelse:\n    print(f\"Qwen3-8B-4bit already downloaded at {qwen_path}\")\n\n# Download Llama 3B (~2 GB) ‚Äî for comparison\nllama_path = os.path.join(MODELS_DIR, \"Llama-3.2-3B-Instruct-4bit\")\nif not os.path.exists(llama_path):\n    print(\"Downloading Llama-3.2-3B-Instruct-4bit (~2 GB)...\")\n    snapshot_download(\n        repo_id=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n        local_dir=llama_path,\n    )\n    print(f\"Saved to: {llama_path}\")\nelse:\n    print(f\"Llama-3.2-3B-Instruct-4bit already downloaded at {llama_path}\")\n\nprint(\"\\nBoth models ready! ‚úì\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n## Experiment 1: Your First Local LLM\n\n**Goal:** See a model running on YOUR machine. No cloud, no API.\n\nModels are loaded from `./models/` ‚Äî no internet needed after download."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from mlx_lm import load, generate\n\n# Load from local path (no internet needed)\nmodel, tokenizer = load(qwen_path)\nprint(\"Qwen 8B loaded! ‚úì\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate your first response!\n# Note: Qwen 3 has a \"thinking\" mode ‚Äî it reasons internally before answering.\n# Adding /no_think to the prompt disables it for faster, direct answers.\n\nresponse = generate(\n    model,\n    tokenizer,\n    prompt=\"What is Rust programming language in one sentence? /no_think\",\n    max_tokens=100,\n    verbose=True\n)\nprint(\"\\n\" + response)"
  },
  {
   "cell_type": "code",
   "source": "# Now try WITH thinking mode ‚Äî same question, no /no_think\n# You'll see the model \"reason\" internally before answering.\n# It needs more tokens because the thinking eats into the budget.\n\nresponse = generate(\n    model,\n    tokenizer,\n    prompt=\"What is Rust programming language in one sentence?\",\n    max_tokens=500,  # more room for thinking + answer\n    verbose=True\n)\nprint(\"\\n\" + response)\n# Look for the <think>...</think> section ‚Äî that's the model reasoning!",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Experiment 1b: 3B vs 8B ‚Äî Head to Head\n\nBoth models get the **exact same prompts**. We measure:\n- **Speed** (tokens/sec) ‚Äî how fast each generates\n- **Quality** ‚Äî you judge which answer is better  \n- **Memory** ‚Äî how much RAM each uses\n\nThis is how you'd pick a model for Jarvis in the real world."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the small model ‚Äî measure load time for both\nfrom mlx.utils import tree_flatten\nimport time\n\ndef count_params(m):\n    return sum(v.size for _, v in tree_flatten(m.parameters()))\n\n# Re-load 8B to measure load time (already in memory, but we time it)\nstart = time.time()\nmodel, tokenizer = load(qwen_path)\nload_time_8b = time.time() - start\nprint(f\"Qwen 8B loaded in {load_time_8b:.1f}s ‚úì\")\n\nstart = time.time()\nmodel_small, tok_small = load(llama_path)\nload_time_3b = time.time() - start\nprint(f\"Llama 3B loaded in {load_time_3b:.1f}s ‚úì\")\n\n# Architecture comparison\nparams_8b = count_params(model)\nparams_3b = count_params(model_small)\nlayers_8b = len(model.model.layers)\nlayers_3b = len(model_small.model.layers)\n\nprint(f\"\\n{'':30s} {'Qwen 8B':>12s}  {'Llama 3B':>12s}\")\nprint(\"-\" * 58)\nprint(f\"{'Parameters':30s} {params_8b:>12,}  {params_3b:>12,}\")\nprint(f\"{'Layers':30s} {layers_8b:>12}  {layers_3b:>12}\")\nprint(f\"{'Embedding dims':30s} {model.model.embed_tokens.weight.shape[1]:>12,}  {model_small.model.embed_tokens.weight.shape[1]:>12,}\")\nprint(f\"{'Vocab size':30s} {model.model.embed_tokens.weight.shape[0]:>12,}  {model_small.model.embed_tokens.weight.shape[0]:>12,}\")\nprint(f\"{'Model load time':30s} {load_time_8b:>11.1f}s  {load_time_3b:>11.1f}s\")\nprint(f\"{'Est. RAM (Q4)':30s} {'~5.0 GB':>12s}  {'~2.0 GB':>12s}\")\nprint(f\"\\n‚Üí 8B has ~{params_8b/params_3b:.1f}√ó more parameters = more 'knowledge' baked into the weights.\")"
  },
  {
   "cell_type": "code",
   "source": "import time\n\n# ‚îÄ‚îÄ Benchmark: same prompts, both models, measure everything ‚îÄ‚îÄ\n\ntest_prompts = [\n    {\n        \"name\": \"Simple fact\",\n        \"prompt\": \"What is the capital of Japan? Answer in one sentence. /no_think\",\n        \"max_tokens\": 50,\n    },\n    {\n        \"name\": \"Summarization\",\n        \"prompt\": \"Summarize in 2 bullet points: Transformers use self-attention to process all tokens in parallel. Unlike RNNs, they don't need to process tokens sequentially. The attention mechanism lets each token look at every other token, which makes training very efficient on GPUs. /no_think\",\n        \"max_tokens\": 100,\n    },\n    {\n        \"name\": \"Reasoning\",\n        \"prompt\": \"If a train travels 120 km in 2 hours, and then 90 km in 1.5 hours, what was its average speed for the entire journey? Show your work briefly. /no_think\",\n        \"max_tokens\": 150,\n    },\n    {\n        \"name\": \"Code explanation\",\n        \"prompt\": \"Explain what this Python code does: sorted(set(words), key=lambda w: len(w), reverse=True)[:5] /no_think\",\n        \"max_tokens\": 100,\n    },\n    {\n        \"name\": \"Creative writing\",\n        \"prompt\": \"Write a 2-sentence horror story about a smart home assistant. /no_think\",\n        \"max_tokens\": 80,\n    },\n]\n\nmodels_to_test = [\n    (\"Qwen 8B\", model, tokenizer),\n    (\"Llama 3B\", model_small, tok_small),\n]\n\n# Store results for summary table\nresults = []\n\nfor test in test_prompts:\n    print(f\"\\n{'='*70}\")\n    print(f\"TEST: {test['name']}\")\n    print(f\"PROMPT: {test['prompt'][:80]}...\")\n    print(f\"{'='*70}\")\n    \n    for model_name, m, tok in models_to_test:\n        start = time.time()\n        response = generate(m, tok, prompt=test[\"prompt\"], max_tokens=test[\"max_tokens\"], verbose=False)\n        elapsed = time.time() - start\n        \n        prompt_tokens = len(tok.encode(test[\"prompt\"]))\n        out_tokens = len(tok.encode(response))\n        tps = out_tokens / elapsed if elapsed > 0 else 0\n        \n        results.append({\n            \"test\": test[\"name\"],\n            \"model\": model_name,\n            \"prompt_tokens\": prompt_tokens,\n            \"output_tokens\": out_tokens,\n            \"time\": elapsed,\n            \"tps\": tps,\n            \"response\": response.strip(),\n        })\n        \n        print(f\"\\n  [{model_name}] ({out_tokens} tokens, {tps:.1f} tok/s, {elapsed:.1f}s)\")\n        print(f\"  {response.strip()[:300]}\")\n\n\n# ‚îÄ‚îÄ Per-test comparison table ‚îÄ‚îÄ\nprint(f\"\\n\\n{'='*80}\")\nprint(f\"  RESULTS: Per-Test Comparison\")\nprint(f\"{'='*80}\")\nprint(f\"\\n{'TEST':<18} {'MODEL':<12} {'IN TOK':>6} {'OUT TOK':>7} {'TIME':>7} {'TOK/S':>7}\")\nprint(\"-\" * 62)\nfor r in results:\n    print(f\"{r['test']:<18} {r['model']:<12} {r['prompt_tokens']:>6} {r['output_tokens']:>7} {r['time']:>6.1f}s {r['tps']:>6.1f}\")\n\n\n# ‚îÄ‚îÄ Aggregated comparison table ‚îÄ‚îÄ\nprint(f\"\\n\\n{'='*80}\")\nprint(f\"  RESULTS: Aggregated Comparison\")\nprint(f\"{'='*80}\")\n\nprint(f\"\\n{'METRIC':<35} {'Qwen 8B':>12} {'Llama 3B':>12} {'Winner':>10}\")\nprint(\"-\" * 72)\n\nfor model_name in [\"Qwen 8B\", \"Llama 3B\"]:\n    locals()[f\"r_{model_name.split()[0].lower()}\"] = [r for r in results if r[\"model\"] == model_name]\n\nr8 = [r for r in results if r[\"model\"] == \"Qwen 8B\"]\nr3 = [r for r in results if r[\"model\"] == \"Llama 3B\"]\n\n# Load time\nprint(f\"{'Model load time':<35} {load_time_8b:>11.1f}s {load_time_3b:>11.1f}s {'3B':>10}\")\n\n# Avg generation time\navg_time_8b = sum(r[\"time\"] for r in r8) / len(r8)\navg_time_3b = sum(r[\"time\"] for r in r3) / len(r3)\nprint(f\"{'Avg generation time':<35} {avg_time_8b:>11.1f}s {avg_time_3b:>11.1f}s {'3B' if avg_time_3b < avg_time_8b else '8B':>10}\")\n\n# Total generation time\ntotal_time_8b = sum(r[\"time\"] for r in r8)\ntotal_time_3b = sum(r[\"time\"] for r in r3)\nprint(f\"{'Total time (all 5 tests)':<35} {total_time_8b:>11.1f}s {total_time_3b:>11.1f}s {'3B' if total_time_3b < total_time_8b else '8B':>10}\")\n\n# Avg tokens/sec\navg_tps_8b = sum(r[\"tps\"] for r in r8) / len(r8)\navg_tps_3b = sum(r[\"tps\"] for r in r3) / len(r3)\nprint(f\"{'Avg speed (tok/s)':<35} {avg_tps_8b:>11.1f} {avg_tps_3b:>11.1f} {'3B' if avg_tps_3b > avg_tps_8b else '8B':>10}\")\n\n# Total tokens produced\ntotal_tok_8b = sum(r[\"output_tokens\"] for r in r8)\ntotal_tok_3b = sum(r[\"output_tokens\"] for r in r3)\nprint(f\"{'Total tokens produced':<35} {total_tok_8b:>12} {total_tok_3b:>12} {'‚Äî':>10}\")\n\n# Avg tokens per response\navg_tok_8b = total_tok_8b / len(r8)\navg_tok_3b = total_tok_3b / len(r3)\nprint(f\"{'Avg tokens per response':<35} {avg_tok_8b:>11.1f} {avg_tok_3b:>11.1f} {'‚Äî':>10}\")\n\n# Parameters\nprint(f\"{'Parameters':<35} {params_8b:>12,} {params_3b:>12,} {'‚Äî':>10}\")\n\n# RAM\nprint(f\"{'Est. RAM usage (Q4)':<35} {'~5.0 GB':>12} {'~2.0 GB':>12} {'3B':>10}\")\n\n# Speed advantage\nspeed_ratio = avg_tps_3b / avg_tps_8b if avg_tps_8b > 0 else 0\nprint(f\"\\n  Speed advantage:  3B is ~{speed_ratio:.1f}√ó faster\")\nprint(f\"  Size advantage:   3B uses ~{5.0/2.0:.1f}√ó less RAM\")\nprint(f\"  Quality:          YOU judge from the side-by-side below ‚Üì\")\n\nprint(f\"\\nDone! Run next cells for side-by-side quality comparison.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ‚îÄ‚îÄ Summary Table ‚îÄ‚îÄ\n\nprint(f\"\\n{'TEST':<18} {'MODEL':<12} {'TOKENS':>6} {'TIME':>7} {'TOK/S':>7}\")\nprint(\"-\" * 55)\n\nfor r in results:\n    print(f\"{r['test']:<18} {r['model']:<12} {r['tokens']:>6} {r['time']:>6.1f}s {r['tps']:>6.1f}\")\n\n# Averages\nfor model_name in [\"Qwen 8B\", \"Llama 3B\"]:\n    model_results = [r for r in results if r[\"model\"] == model_name]\n    avg_tps = sum(r[\"tps\"] for r in model_results) / len(model_results)\n    avg_time = sum(r[\"time\"] for r in model_results) / len(model_results)\n    total_tokens = sum(r[\"tokens\"] for r in model_results)\n    print(f\"\\n  {model_name} average: {avg_tps:.1f} tok/s, {avg_time:.1f}s per response, {total_tokens} total tokens\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ‚îÄ‚îÄ Side-by-side responses ‚Äî read and judge quality yourself ‚îÄ‚îÄ\n\nprint(\"SIDE-BY-SIDE: Read both responses and judge which is better.\\n\")\nprint(\"Score each pair: 8B wins / tie / 3B wins\\n\")\n\ntest_names = list(dict.fromkeys(r[\"test\"] for r in results))  # unique, ordered\n\nfor test_name in test_names:\n    pair = [r for r in results if r[\"test\"] == test_name]\n    r8b = next(r for r in pair if r[\"model\"] == \"Qwen 8B\")\n    r3b = next(r for r in pair if r[\"model\"] == \"Llama 3B\")\n    \n    print(f\"{'='*70}\")\n    print(f\"  {test_name.upper()}\")\n    print(f\"{'='*70}\")\n    print(f\"\\n  Qwen 8B ({r8b['tps']:.0f} tok/s):\")\n    print(f\"  {r8b['response'][:400]}\")\n    print(f\"\\n  Llama 3B ({r3b['tps']:.0f} tok/s):\")\n    print(f\"  {r3b['response'][:400]}\")\n    \n    speed_winner = \"3B\" if r3b[\"tps\"] > r8b[\"tps\"] else \"8B\"\n    speed_diff = abs(r3b[\"tps\"] - r8b[\"tps\"]) / min(r3b[\"tps\"], r8b[\"tps\"]) * 100\n    print(f\"\\n  Speed winner: {speed_winner} ({speed_diff:.0f}% faster)\")\n    print()\n\nprint(\"\\n‚Üí Typical pattern: 3B is faster, 8B gives better quality.\")\nprint(\"‚Üí For Jarvis: use 8B for important tasks (summarize, extract),\")\nprint(\"  3B for quick/simple tasks (short replies, classification).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ‚îÄ‚îÄ Memory comparison ‚îÄ‚îÄ\nimport subprocess, os\n\ndef get_process_memory_mb():\n    \"\"\"Get current process RSS in MB (macOS).\"\"\"\n    result = subprocess.run(\n        [\"ps\", \"-o\", \"rss=\", \"-p\", str(os.getpid())],\n        capture_output=True, text=True\n    )\n    return int(result.stdout.strip()) / 1024\n\nmem_now = get_process_memory_mb()\n\n# Rough estimates based on model file sizes\nprint(\"Memory Comparison\")\nprint(\"=\" * 50)\nprint(f\"{'Qwen 8B (Q4)':<25} ~5.0 GB weights in RAM\")\nprint(f\"{'Llama 3B (Q4)':<25} ~2.0 GB weights in RAM\")\nprint(f\"{'Both loaded (current)':<25} ~{mem_now/1024:.1f} GB process RSS\")\nprint()\nprint(\"For Jarvis (single model):\")\nprint(f\"  8B only: ~5 GB + OS overhead ‚Üí ~8 GB total\")\nprint(f\"  3B only: ~2 GB + OS overhead ‚Üí ~5 GB total\")\nprint(f\"  Your RAM: 36 GB ‚Üí plenty of room either way\")\nprint()\nprint(\"‚Üí Memory isn't the bottleneck on 36 GB.\")\nprint(\"‚Üí The real question is: quality vs speed.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: See the Tokenizer\n",
    "\n",
    "**Goal:** Use a REAL tokenizer (not our mock one). See how it actually splits text.\n",
    "\n",
    "Remember from our React app: tokenizer splits text ‚Üí assigns IDs ‚Üí embeddings look up those IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "text = \"Rust is a programming language\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Number of tokens: {len(token_ids)}\")\n",
    "print()\n",
    "\n",
    "# Decode each token to see the pieces\n",
    "for i, tid in enumerate(token_ids):\n",
    "    piece = tokenizer.decode([tid])\n",
    "    print(f\"  Token {i}: ID {tid:>6} ‚Üí '{piece}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer surprises ‚Äî run this and see what's unexpected!\n",
    "tests = [\n",
    "    \"Hello\",             # one token? or two?\n",
    "    \"hello\",             # lowercase ‚Äî same token?\n",
    "    \"Rust\",              # capital R\n",
    "    \"rust\",              # lowercase r  \n",
    "    \"tokenization\",      # long word ‚Äî how many pieces?\n",
    "    \"MLX\",               # acronym\n",
    "    \"üî•\",                # emoji\n",
    "    \"   spaces   \",      # whitespace\n",
    "    \"don't\",             # apostrophe\n",
    "    \"ChatGPT\",           # compound word\n",
    "]\n",
    "\n",
    "print(f\"{'Input':<20} {'Tokens':>3}  Pieces\")\n",
    "print(\"-\" * 60)\n",
    "for t in tests:\n",
    "    ids = tokenizer.encode(t)\n",
    "    pieces = [tokenizer.decode([i]) for i in ids]\n",
    "    print(f\"'{t}'\" + \" \" * (18 - len(t)) + f\"{len(ids):>3}  {pieces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY INSIGHT: same word, same token ID ‚Äî tokenizer has NO idea about meaning\n",
    "# \"Rust\" the language and \"Rust\" the corrosion get the SAME token ID\n",
    "\n",
    "text1 = \"Rust is a programming language\"\n",
    "text2 = \"Rust on the iron door needs cleaned\"\n",
    "\n",
    "ids1 = tokenizer.encode(text1)\n",
    "ids2 = tokenizer.encode(text2)\n",
    "\n",
    "print(f\"'{text1}' ‚Üí first token ID: {ids1[0]}\")\n",
    "print(f\"'{text2}' ‚Üí first token ID: {ids2[0]}\")\n",
    "print(f\"\\nSame ID? {ids1[0] == ids2[0]}\")\n",
    "print(\"\\n‚Üí Context (programming vs corrosion) comes from ATTENTION, not the tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big is the vocabulary?\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n",
    "print()\n",
    "\n",
    "# Peek at some vocab entries\n",
    "sample_ids = [0, 1, 2, 100, 1000, 5000, 10000, 50000, 100000]\n",
    "for i in sample_ids:\n",
    "    if i < tokenizer.vocab_size:\n",
    "        print(f\"  Token {i:>6}: '{tokenizer.decode([i])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: See the Embeddings\n",
    "\n",
    "**Goal:** Look at REAL embedding vectors ‚Äî 4,096 numbers per token (our React app showed 8 fake ones).\n",
    "\n",
    "This is the embedding TABLE LOOKUP from our visualization: token ID ‚Üí row in table ‚Üí 4,096 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "# Get the embedding table ‚Äî the ACTUAL weight matrix\n",
    "embed_table = model.model.embed_tokens\n",
    "\n",
    "print(f\"Embedding table shape: {embed_table.weight.shape}\")\n",
    "print(f\"  ‚Üí {embed_table.weight.shape[0]:,} tokens √ó {embed_table.weight.shape[1]:,} dimensions\")\n",
    "print(f\"  ‚Üí Each token maps to a vector of {embed_table.weight.shape[1]:,} numbers\")\n",
    "print(f\"  ‚Üí This is the table we visualized in the React app (but 4,096 dims instead of 8)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count parameters by group ‚Äî match our \"Inside the File\" numbers\n\nfrom mlx.utils import tree_flatten\n\ndef count_params(module):\n    return sum(v.size for _, v in tree_flatten(module.parameters()))\n\nembed_params = model.model.embed_tokens.weight.size\nlayer_params = count_params(model.model.layers[0])\ntotal_layer_params = sum(count_params(l) for l in model.model.layers)\nhead_params = model.lm_head.weight.size\nn_layers = len(model.model.layers)\n\nprint(\"Parameter Count by Group\")\nprint(\"=\" * 50)\nprint(f\"Embedding Table:     {embed_params:>14,}\")\nprint(f\"Per Layer:           {layer_params:>14,}\")\nprint(f\"√ó {n_layers} Layers:          {total_layer_params:>14,}\")\nprint(f\"Prediction Head:     {head_params:>14,}\")\nprint(\"-\" * 50)\ntotal = embed_params + total_layer_params + head_params\nprint(f\"Total:               {total:>14,}\")\nprint(f\"\\n‚Üí This is why it's called an '8B' model!\")\nprint(f\"‚Üí In Q4 quantization, each param ‚âà 0.5 bytes ‚Üí ~{total * 0.5 / 1e9:.1f} GB file\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROOF: Same word = same embedding (before attention)\n",
    "# This is what we showed in the React app!\n",
    "\n",
    "text1 = \"Rust is a programming language\"\n",
    "text2 = \"Rust on the iron door\"\n",
    "\n",
    "ids1 = tokenizer.encode(text1)\n",
    "ids2 = tokenizer.encode(text2)\n",
    "\n",
    "emb1 = embed_table(mx.array([ids1]))\n",
    "emb2 = embed_table(mx.array([ids2]))\n",
    "mx.eval(emb1, emb2)\n",
    "\n",
    "# Compare the \"Rust\" embedding from both sentences\n",
    "rust_emb1 = emb1[0, 0, :]  # first token of sentence 1\n",
    "rust_emb2 = emb2[0, 0, :]  # first token of sentence 2\n",
    "\n",
    "diff = mx.sum(mx.abs(rust_emb1 - rust_emb2)).item()\n",
    "\n",
    "print(f\"'Rust' embedding from '{text1}'\")\n",
    "print(f\"  First 5 values: {rust_emb1[:5].tolist()}\")\n",
    "print(f\"\\n'Rust' embedding from '{text2}'\")\n",
    "print(f\"  First 5 values: {rust_emb2[:5].tolist()}\")\n",
    "print(f\"\\nTotal difference: {diff}\")\n",
    "print(f\"\\n‚Üí Difference is 0.0 because embeddings are just a TABLE LOOKUP.\")\n",
    "print(f\"‚Üí Same token ID = same 4,096 numbers, regardless of context.\")\n",
    "print(f\"‚Üí The 32 attention layers AFTER this create contextual understanding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: See the Weights\n",
    "\n",
    "**Goal:** Open the model and look at the weight groups from our \"Inside the File\" visualization.\n",
    "\n",
    "Remember the anatomy:\n",
    "- Embedding Table\n",
    "- 32 Layers (each: Attention Weights + Transform Weights)\n",
    "- Prediction Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight groups ‚Äî exactly what we visualized!\n",
    "\n",
    "# 1. EMBEDDING TABLE\n",
    "print(\"1. EMBEDDING TABLE\")\n",
    "print(f\"   Shape: {model.model.embed_tokens.weight.shape}\")\n",
    "print(f\"   ‚Üí {model.model.embed_tokens.weight.shape[0]:,} vocab √ó {model.model.embed_tokens.weight.shape[1]:,} dims\")\n",
    "print()\n",
    "\n",
    "# 2. LAYERS\n",
    "print(f\"2. TRANSFORMER LAYERS: {len(model.model.layers)} layers\")\n",
    "layer0 = model.model.layers[0]\n",
    "print(f\"\\n   Layer 0 ‚Äî Attention Weights:\")\n",
    "print(f\"     Q (query):  {layer0.self_attn.q_proj.weight.shape}\")\n",
    "print(f\"     K (key):    {layer0.self_attn.k_proj.weight.shape}\")\n",
    "print(f\"     V (value):  {layer0.self_attn.v_proj.weight.shape}\")\n",
    "print(f\"     O (output): {layer0.self_attn.o_proj.weight.shape}\")\n",
    "\n",
    "print(f\"\\n   Layer 0 ‚Äî Transform (Feed-Forward) Weights:\")\n",
    "print(f\"     Gate: {layer0.mlp.gate_proj.weight.shape}\")\n",
    "print(f\"     Up:   {layer0.mlp.up_proj.weight.shape}\")\n",
    "print(f\"     Down: {layer0.mlp.down_proj.weight.shape}\")\n",
    "print()\n",
    "\n",
    "# 3. PREDICTION HEAD\n",
    "print(\"3. PREDICTION HEAD\")\n",
    "print(f\"   Shape: {model.lm_head.weight.shape}\")\n",
    "print(f\"   ‚Üí Maps {model.lm_head.weight.shape[1]:,} hidden dims ‚Üí {model.lm_head.weight.shape[0]:,} vocab scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters by group ‚Äî match our \"Inside the File\" numbers\n",
    "\n",
    "def count_params(module):\n",
    "    return sum(p.size for p in module.parameters())\n",
    "\n",
    "embed_params = model.model.embed_tokens.weight.size\n",
    "layer_params = count_params(model.model.layers[0])\n",
    "total_layer_params = sum(count_params(l) for l in model.model.layers)\n",
    "head_params = model.lm_head.weight.size\n",
    "n_layers = len(model.model.layers)\n",
    "\n",
    "print(\"Parameter Count by Group\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Embedding Table:     {embed_params:>14,}\")\n",
    "print(f\"Per Layer:           {layer_params:>14,}\")\n",
    "print(f\"√ó {n_layers} Layers:          {total_layer_params:>14,}\")\n",
    "print(f\"Prediction Head:     {head_params:>14,}\")\n",
    "print(\"-\" * 50)\n",
    "total = embed_params + total_layer_params + head_params\n",
    "print(f\"Total:               {total:>14,}\")\n",
    "print(f\"\\n‚Üí This is why it's called an '8B' model!\")\n",
    "print(f\"‚Üí In Q4 quantization, each param ‚âà 0.5 bytes ‚Üí ~{total * 0.5 / 1e9:.1f} GB file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at actual weight values\n",
    "# These are the numbers that were learned during training!\n",
    "\n",
    "q_weight = layer0.self_attn.q_proj.weight\n",
    "print(f\"Query weight matrix shape: {q_weight.shape}\")\n",
    "print(f\"dtype: {q_weight.dtype}\")\n",
    "print(f\"\\nSample 5√ó5 corner of the Q weight matrix:\")\n",
    "\n",
    "sample = q_weight[:5, :5]\n",
    "mx.eval(sample)\n",
    "for row in sample.tolist():\n",
    "    print(f\"  [{', '.join(f'{v:+.4f}' for v in row)}]\")\n",
    "\n",
    "print(f\"\\n‚Üí These specific numbers were learned from trillions of words of text.\")\n",
    "print(f\"‚Üí Change any one and the model behaves slightly differently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5: Watch Attention\n",
    "\n",
    "**Goal:** Run text through the model and see Q, K, V ‚Äî the attention mechanism we visualized.\n",
    "\n",
    "Remember: Q = \"what am I looking for?\", K = \"what do I contain?\", V = \"what info do I share?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings, then pass through first layer's attention\n",
    "\n",
    "text = \"Rust is a programming language\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "tokens = [tokenizer.decode([t]) for t in token_ids]\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Step 1: Embed\n",
    "input_ids = mx.array([token_ids])\n",
    "hidden = model.model.embed_tokens(input_ids)\n",
    "print(f\"After embedding: {hidden.shape}\")\n",
    "\n",
    "# Step 2: Normalize (models do this before attention)\n",
    "layer0 = model.model.layers[0]\n",
    "normed = layer0.input_layernorm(hidden)\n",
    "\n",
    "# Step 3: Compute Q, K, V\n",
    "q = layer0.self_attn.q_proj(normed)\n",
    "k = layer0.self_attn.k_proj(normed)\n",
    "v = layer0.self_attn.v_proj(normed)\n",
    "mx.eval(q, k, v)\n",
    "\n",
    "print(f\"\\nQ (query) shape:  {q.shape}  ‚Äî 'what am I looking for?'\")\n",
    "print(f\"K (key) shape:    {k.shape}  ‚Äî 'what do I contain?'\")\n",
    "print(f\"V (value) shape:  {v.shape}  ‚Äî 'what info do I share?'\")\n",
    "\n",
    "print(f\"\\n‚Üí These are the REAL Q/K/V from our attention visualization!\")\n",
    "print(f\"‚Üí The model multiplies Q√óK to get attention scores (who looks at whom).\")\n",
    "print(f\"‚Üí Then uses scores to weight-sum V (gather information from attended tokens).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention scores manually for layer 0\n",
    "# This is: scores = Q √ó K^T / sqrt(d_k)\n",
    "\n",
    "import math\n",
    "\n",
    "d_k = q.shape[-1]\n",
    "scores = (q @ mx.transpose(k, (0, 1, 3, 2))) / math.sqrt(d_k) if len(q.shape) == 4 else (q @ k.T) / math.sqrt(d_k)\n",
    "mx.eval(scores)\n",
    "\n",
    "print(f\"Attention scores shape: {scores.shape}\")\n",
    "print(f\"\\nThese scores tell us: for each token, how much does it 'attend to' every other token?\")\n",
    "print(f\"Higher score = pays more attention to that token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 6: See Prediction Probabilities\n",
    "\n",
    "**Goal:** Give the model a partial sentence and see the probability distribution ‚Äî exactly like our PredictionPanel.\n",
    "\n",
    "The model outputs a score for EVERY word in its vocabulary. Softmax turns scores ‚Üí probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What comes after \"Rust is a programming\"?\n",
    "\n",
    "prompt = \"Rust is a programming\"\n",
    "token_ids = tokenizer.encode(prompt)\n",
    "input_ids = mx.array([token_ids])\n",
    "\n",
    "# Forward pass ‚Äî runs through ALL layers\n",
    "logits = model(input_ids)\n",
    "mx.eval(logits)\n",
    "\n",
    "# Get prediction for the LAST position\n",
    "last_logits = logits[0, -1, :]  # shape: (vocab_size,)\n",
    "print(f\"Raw logits shape: {last_logits.shape}\")\n",
    "print(f\"‚Üí One score for each of {last_logits.shape[0]:,} tokens in vocabulary\")\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = mx.softmax(last_logits)\n",
    "mx.eval(probs)\n",
    "\n",
    "# Top 10 predictions\n",
    "top_indices = mx.argsort(probs)[-10:][::-1]\n",
    "mx.eval(top_indices)\n",
    "\n",
    "print(f\"\\nTop 10 predictions for '{prompt} ___':\\n\")\n",
    "for idx in top_indices.tolist():\n",
    "    token_text = tokenizer.decode([idx])\n",
    "    prob = probs[idx].item()\n",
    "    bar = \"‚ñà\" * int(prob * 50)\n",
    "    print(f\"  {prob:6.2%}  '{token_text}'  {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: confident vs uncertain predictions\n",
    "\n",
    "prompts = [\n",
    "    \"Rust is a programming\",         # ‚Üí \"language\" (high confidence)\n",
    "    \"The capital of France is\",       # ‚Üí \"Paris\" (very high confidence)\n",
    "    \"I love eating\",                  # ‚Üí many foods (spread out)\n",
    "    \"Rust on the iron\",               # ‚Üí multiple options\n",
    "    \"The meaning of life is\",         # ‚Üí very uncertain\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    ids = tokenizer.encode(prompt)\n",
    "    logits = model(mx.array([ids]))\n",
    "    probs = mx.softmax(logits[0, -1, :])\n",
    "    top5 = mx.argsort(probs)[-5:][::-1]\n",
    "    mx.eval(probs, top5)\n",
    "\n",
    "    top1_prob = probs[top5[0]].item()\n",
    "    confidence = \"HIGH\" if top1_prob > 0.5 else \"MEDIUM\" if top1_prob > 0.2 else \"LOW\"\n",
    "\n",
    "    print(f\"\\n'{prompt} ___'  [{confidence} confidence]\")\n",
    "    for idx in top5.tolist():\n",
    "        p = probs[idx].item()\n",
    "        t = tokenizer.decode([idx])\n",
    "        bar = \"‚ñà\" * int(p * 30)\n",
    "        print(f\"  {p:6.2%}  '{t}'  {bar}\")\n",
    "\n",
    "print(\"\\n‚Üí When confident, one token dominates (90%+).\")\n",
    "print(\"‚Üí When uncertain, probability spreads across many tokens.\")\n",
    "print(\"‚Üí Temperature & top-p control how we PICK from this distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 7: Jarvis Model Selection\n",
    "\n",
    "**Goal:** Test the model with Jarvis-style prompts (summarize, extract, reply)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Jarvis-style tasks\n",
    "jarvis_tasks = [\n",
    "    {\n",
    "        \"name\": \"Summarize content\",\n",
    "        \"prompt\": \"Summarize this in 3 bullet points: Transformers are a type of neural network architecture that uses self-attention mechanisms. Unlike RNNs which process sequences step by step, transformers process all tokens in parallel. The key innovation is the attention mechanism which allows each token to look at all other tokens when computing its representation. This makes transformers very efficient for training on GPUs and has led to models like GPT, BERT, and LLaMA.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Draft email reply\",\n",
    "        \"prompt\": \"Write a short, friendly reply to this email: 'Hey Ankit, can we reschedule our Tuesday meeting to Thursday same time? I have a conflict that came up.'\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Extract key points\",\n",
    "        \"prompt\": \"Extract the 3 most important facts from this: Apple's M3 Pro chip features an 11-core CPU with 5 performance and 6 efficiency cores. It has 18-core GPU and supports up to 36GB of unified memory. The chip is built on 3nm process technology and delivers up to 40% faster CPU performance compared to M1 Pro.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Explain code\",\n",
    "        \"prompt\": \"Explain what this Rust code does in plain English: fn fibonacci(n: u32) -> u32 { match n { 0 => 0, 1 => 1, _ => fibonacci(n-1) + fibonacci(n-2) } }\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for task in jarvis_tasks:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"TASK: {task['name']}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    response = generate(model, tokenizer, prompt=task['prompt'], max_tokens=200, verbose=True)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(response)\n",
    "    print(f\"\\n‚è±  {elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed benchmark\n",
    "import time\n",
    "\n",
    "prompt = \"Write a detailed paragraph about the benefits of local AI models for privacy and performance.\"\n",
    "\n",
    "start = time.time()\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens=300, verbose=True)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Count output tokens\n",
    "output_tokens = len(tokenizer.encode(response))\n",
    "\n",
    "print(f\"\\n{'=' * 40}\")\n",
    "print(f\"Output tokens: {output_tokens}\")\n",
    "print(f\"Total time: {elapsed:.1f}s\")\n",
    "print(f\"Speed: {output_tokens/elapsed:.1f} tokens/sec\")\n",
    "print(f\"\\n‚Üí For Jarvis: a typical response (50-100 tokens) takes {50/max(output_tokens/elapsed, 1):.1f}-{100/max(output_tokens/elapsed, 1):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 8: Quantization ‚Äî See the Trade-off\n",
    "\n",
    "**Goal:** Understand why Q4 works. Each weight stored in 4 bits instead of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check memory usage\n",
    "pid = os.getpid()\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.size for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\\nIf stored at full precision (float32 = 4 bytes each):\")\n",
    "print(f\"  {total_params * 4 / 1e9:.1f} GB\")\n",
    "print(f\"\\nIf stored at half precision (float16 = 2 bytes each):\")\n",
    "print(f\"  {total_params * 2 / 1e9:.1f} GB\")\n",
    "print(f\"\\nAt Q4 quantization (4 bits = 0.5 bytes each):\")\n",
    "print(f\"  {total_params * 0.5 / 1e9:.1f} GB\")\n",
    "print(f\"\\n‚Üí Quantization shrinks the model ~4√ó with minimal quality loss.\")\n",
    "print(f\"‚Üí That's why a 8B model fits comfortably on your 36 GB Mac!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at quantized weight dtype\n",
    "layer0 = model.model.layers[0]\n",
    "q_weight = layer0.self_attn.q_proj.weight\n",
    "\n",
    "print(f\"Weight dtype: {q_weight.dtype}\")\n",
    "print(f\"Weight shape: {q_weight.shape}\")\n",
    "print(f\"\\n‚Üí The values are packed in 4-bit format.\")\n",
    "print(f\"‚Üí During inference, they get unpacked to float16/float32 for computation.\")\n",
    "print(f\"‚Üí Small precision loss, big memory savings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What Connects to Our React App\n",
    "\n",
    "| React App (mock data) | MLX Experiment (real data) |\n",
    "|---|---|\n",
    "| TokenizerPanel (simple split) | Exp 2: Real BPE tokenizer with 151k vocab |\n",
    "| EmbeddingPanel (8 fake dims) | Exp 3: Real 4,096-dim embeddings |\n",
    "| \"Inside the File\" (diagrams) | Exp 4: Actual weight shapes and counts |\n",
    "| AttentionPanel (mock scores) | Exp 5: Real Q/K/V matrices |\n",
    "| PredictionPanel (fake top-5) | Exp 6: Real probability distribution |\n",
    "| AutoregressiveDemo | Exp 1: Watch real token-by-token generation |\n",
    "| KV Cache Demo (theoretical) | Exp 1: Actual tokens/sec (cache is built-in) |\n",
    "\n",
    "**Next steps after these experiments:**\n",
    "1. Integrate Qwen 8B into Jarvis via MLX-LM Python API\n",
    "2. Build Part 3 of React app (Model Sizes, Quantization, Memory) using real data from these experiments\n",
    "3. Explore fine-tuning with LoRA (teach the model Jarvis-specific behavior)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}