{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Qwen2.5-Omni Test\n",
    "\n",
    "Test the **official** HuggingFace Qwen2.5-Omni with the same Jarvis audio.\n",
    "If this works but MLX fails → the mlx-lm-omni port is broken.\n",
    "If this also fails → the audio characteristics are the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install transformers accelerate soundfile librosa torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with official Qwen2.5-Omni via transformers\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Audio files\n",
    "JFK_AUDIO = 'test_audio 11.04.46\\u202fPM.wav'\n",
    "JARVIS_AUDIO = 'test_audio.wav'\n",
    "\n",
    "for f in [JFK_AUDIO, JARVIS_AUDIO]:\n",
    "    audio, sr = librosa.load(f, sr=16000)\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    print(f'{f}: {len(audio)/sr:.1f}s | RMS={rms:.4f}')\n",
    "\n",
    "print(f'\\nPyTorch device: {\"mps\" if torch.backends.mps.is_available() else \"cpu\"}')\n",
    "print(f'Torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load official Qwen2.5-Omni-3B from HuggingFace\n# Note: This uses PyTorch (MPS on Apple Silicon), NOT MLX\n\n# Workaround for transformers 5.2.0 bug: VIDEO_PROCESSOR_MAPPING_NAMES\n# has None values that cause TypeError during processor loading\nfrom transformers.models.auto import video_processing_auto\nvideo_processing_auto.VIDEO_PROCESSOR_MAPPING_NAMES = {\n    k: v for k, v in video_processing_auto.VIDEO_PROCESSOR_MAPPING_NAMES.items()\n    if v is not None\n}\n\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n\nMODEL_ID = 'Qwen/Qwen2.5-Omni-3B'\n\nprint(f'Loading {MODEL_ID}...')\nt0 = time.time()\n\nprocessor = Qwen2_5OmniProcessor.from_pretrained(MODEL_ID)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map='auto',  # Will use MPS on Apple Silicon\n)\n\nprint(f'Loaded in {time.time()-t0:.1f}s')\nprint(f'Device: {next(model.parameters()).device}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transcribe both files with the official model\nimport librosa\nimport soundfile as sf\n\ntest_files = [\n    (JFK_AUDIO, 'JFK (baseline)'),\n    (JARVIS_AUDIO, 'Jarvis RAW'),\n]\n\nfor audio_file, label in test_files:\n    audio, sr = librosa.load(audio_file, sr=16000)\n    rms = np.sqrt(np.mean(audio**2))\n    print(f'\\n[{label}] {len(audio)/sr:.1f}s | RMS={rms:.4f}')\n    \n    # Build conversation with audio\n    conversation = [\n        {'role': 'system', 'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]},\n        {\n            'role': 'user',\n            'content': [\n                {'type': 'audio', 'audio': audio_file},\n                {'type': 'text', 'text': 'Transcribe this audio accurately, word for word.'},\n            ],\n        },\n    ]\n    \n    # Process with the official processor\n    text_input = processor.apply_chat_template(\n        conversation, add_generation_prompt=True, tokenize=False\n    )\n    \n    # Load audio as flat 1D mono array — WhisperFeatureExtractor expects (T,)\n    audio_np, _ = sf.read(audio_file, dtype='float32')\n    if audio_np.ndim > 1:\n        audio_np = audio_np.mean(axis=1)  # stereo to mono\n    \n    inputs = processor(\n        text=text_input,\n        audio=[audio_np],  # list of 1D arrays\n        sampling_rate=sr,\n        return_tensors='pt',\n        padding=True,\n    )\n    inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n    \n    t0 = time.time()\n    with torch.no_grad():\n        output_ids = model.generate(**inputs, max_new_tokens=500)\n    elapsed = time.time() - t0\n    \n    # Decode only the generated part\n    input_len = inputs['input_ids'].shape[1]\n    result = processor.decode(output_ids[0][input_len:], skip_special_tokens=True)\n    \n    print(f'  Time: {elapsed:.2f}s')\n    print(f'  Result: {result}')\n    print('=' * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Try with Whisper directly as a sanity check\n",
    "# If Whisper can transcribe it, the audio is fine and the issue is Qwen-specific\n",
    "try:\n",
    "    import whisper\n",
    "    print('Testing with OpenAI Whisper...')\n",
    "    whisper_model = whisper.load_model('base')\n",
    "    \n",
    "    for audio_file, label in [(JFK_AUDIO, 'JFK'), (JARVIS_AUDIO, 'Jarvis')]:\n",
    "        result = whisper_model.transcribe(audio_file)\n",
    "        print(f'\\n[{label}] {result[\"text\"][:200]}')\n",
    "    \n",
    "    del whisper_model\n",
    "except ImportError:\n",
    "    print('Whisper not installed. Run: pip install openai-whisper')\n",
    "    print('Or try mlx-whisper: pip install mlx-whisper')\n",
    "    \n",
    "    # Try mlx-whisper as fallback\n",
    "    try:\n",
    "        import mlx_whisper\n",
    "        print('\\nTesting with mlx-whisper...')\n",
    "        for audio_file, label in [(JFK_AUDIO, 'JFK'), (JARVIS_AUDIO, 'Jarvis')]:\n",
    "            result = mlx_whisper.transcribe(audio_file)\n",
    "            print(f'[{label}] {result[\"text\"][:200]}')\n",
    "    except ImportError:\n",
    "        print('mlx-whisper not installed either. Run: pip install mlx-whisper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "try:\n",
    "    del model, processor\n",
    "except: pass\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "print('Cleaned up.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}