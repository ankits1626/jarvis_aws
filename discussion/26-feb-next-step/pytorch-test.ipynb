{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Qwen2.5-Omni Test\n",
    "\n",
    "Test the **official** HuggingFace Qwen2.5-Omni with the same Jarvis audio.\n",
    "If this works but MLX fails → the mlx-lm-omni port is broken.\n",
    "If this also fails → the audio characteristics are the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install transformers accelerate soundfile librosa torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with official Qwen2.5-Omni via transformers\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Audio files\n",
    "JFK_AUDIO = 'test_audio 11.04.46\\u202fPM.wav'\n",
    "JARVIS_AUDIO = 'test_audio.wav'\n",
    "\n",
    "for f in [JFK_AUDIO, JARVIS_AUDIO]:\n",
    "    audio, sr = librosa.load(f, sr=16000)\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    print(f'{f}: {len(audio)/sr:.1f}s | RMS={rms:.4f}')\n",
    "\n",
    "print(f'\\nPyTorch device: {\"mps\" if torch.backends.mps.is_available() else \"cpu\"}')\n",
    "print(f'Torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load official Qwen2.5-Omni-3B from HuggingFace\n",
    "# Note: This uses PyTorch (MPS on Apple Silicon), NOT MLX\n",
    "from transformers import Qwen2_5OmniModel, Qwen2_5OmniProcessor\n",
    "\n",
    "MODEL_ID = 'Qwen/Qwen2.5-Omni-3B'\n",
    "\n",
    "print(f'Loading {MODEL_ID}...')\n",
    "t0 = time.time()\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_ID)\n",
    "model = Qwen2_5OmniModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',  # Will use MPS on Apple Silicon\n",
    ")\n",
    "\n",
    "print(f'Loaded in {time.time()-t0:.1f}s')\n",
    "print(f'Device: {next(model.parameters()).device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe both files with the official model\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "test_files = [\n",
    "    (JFK_AUDIO, 'JFK (baseline)'),\n",
    "    (JARVIS_AUDIO, 'Jarvis RAW'),\n",
    "]\n",
    "\n",
    "for audio_file, label in test_files:\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    print(f'\\n[{label}] {len(audio)/sr:.1f}s | RMS={rms:.4f}')\n",
    "    \n",
    "    # Build conversation with audio\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'audio', 'audio': audio_file},\n",
    "                {'type': 'text', 'text': 'Transcribe this audio accurately, word for word.'},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Process with the official processor\n",
    "    text_input = processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    \n",
    "    audios, _ = sf.read(audio_file)\n",
    "    if audios.ndim == 1:\n",
    "        audios = [audios]\n",
    "    else:\n",
    "        audios = [audios]\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=text_input,\n",
    "        audios=audios,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    input_len = inputs['input_ids'].shape[1]\n",
    "    result = processor.decode(output_ids[0][input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'  Time: {elapsed:.2f}s')\n",
    "    print(f'  Result: {result}')\n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Try with Whisper directly as a sanity check\n",
    "# If Whisper can transcribe it, the audio is fine and the issue is Qwen-specific\n",
    "try:\n",
    "    import whisper\n",
    "    print('Testing with OpenAI Whisper...')\n",
    "    whisper_model = whisper.load_model('base')\n",
    "    \n",
    "    for audio_file, label in [(JFK_AUDIO, 'JFK'), (JARVIS_AUDIO, 'Jarvis')]:\n",
    "        result = whisper_model.transcribe(audio_file)\n",
    "        print(f'\\n[{label}] {result[\"text\"][:200]}')\n",
    "    \n",
    "    del whisper_model\n",
    "except ImportError:\n",
    "    print('Whisper not installed. Run: pip install openai-whisper')\n",
    "    print('Or try mlx-whisper: pip install mlx-whisper')\n",
    "    \n",
    "    # Try mlx-whisper as fallback\n",
    "    try:\n",
    "        import mlx_whisper\n",
    "        print('\\nTesting with mlx-whisper...')\n",
    "        for audio_file, label in [(JFK_AUDIO, 'JFK'), (JARVIS_AUDIO, 'Jarvis')]:\n",
    "            result = mlx_whisper.transcribe(audio_file)\n",
    "            print(f'[{label}] {result[\"text\"][:200]}')\n",
    "    except ImportError:\n",
    "        print('mlx-whisper not installed either. Run: pip install mlx-whisper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "try:\n",
    "    del model, processor\n",
    "except: pass\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "print('Cleaned up.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}